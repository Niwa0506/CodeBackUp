{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1050f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bcf03b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting janome\n",
      "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7 MB 3.5 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: janome\n",
      "Successfully installed janome-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install janome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04348a6e",
   "metadata": {},
   "source": [
    "# Charfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3afc2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterFilter:\n",
    "    @classmethod\n",
    "    def filter(cls, text:str):\n",
    "        raise NotImplementedError\n",
    "class HtmlStripFilter(CharacterFilter):\n",
    "    @classmethod\n",
    "    def filter(cls,text:str):\n",
    "        html_pattern = re.compile(r\"<[^>]*?>\")\n",
    "        return html_pattern.sub(\"\",text)\n",
    "class LowercaseFilter(CharacterFilter):\n",
    "    @classmethod\n",
    "    def filter(cls,text:str):\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f47cc",
   "metadata": {},
   "source": [
    " # Tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3ca9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "tokennizer = Tokenizer()\n",
    "\n",
    "class BaseTokenizer:\n",
    "    @classmethod\n",
    "    def tokenize(cls,text):\n",
    "        raise NotImplementedError\n",
    "class JanomeTokenizer(BaseTokenizer):\n",
    "    @classmethod \n",
    "    def tokenize(cls,text):\n",
    "        return (t for t in cls.tokenizer.tokenize(text))\n",
    "class WhitespaceTokenizer(BaseTokenizer):\n",
    "    @classmethod\n",
    "    def tokenize(cls,text):\n",
    "        return (t[0]for t in re.finditer(r\"[^\\t\\r\\n]+\",text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62ae16",
   "metadata": {},
   "source": [
    "# TokenFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88d36f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = (\"is\",\"was\",\"to\",\"the\")\n",
    "\n",
    "def is_token_instance(token):\n",
    "    return isinstance(token,Token)\n",
    "\n",
    "class TokenFilter:\n",
    "    @classmethod\n",
    "    def filter(cls,token):\n",
    "        \"\"\"\n",
    "        in: sting or janome.tokenizer.Token\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class StopWordFilter(TokenFilter):\n",
    "    @classmethod\n",
    "    def filter(cls,token):\n",
    "        if isinstance(token,Token):\n",
    "            if token.surface in STOPWORDS:\n",
    "                return None\n",
    "        if token in STOPWORDS:\n",
    "            return None\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adb36c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/niwakazuma/opt/anaconda3/envs/tensorflow/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/niwakazuma/opt/anaconda3/envs/tensorflow/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/niwakazuma/opt/anaconda3/envs/tensorflow/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/niwakazuma/opt/anaconda3/envs/tensorflow/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/niwakazuma/opt/anaconda3/envs/tensorflow/lib/python3.8/site-packages (from nltk) (2022.1.18)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "060b4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "class Stemmer(TokenFilter):\n",
    "    @classmethod\n",
    "    def filter(cls,token: str):\n",
    "        if token:\n",
    "            return ps.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eebfd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSFilter(TokenFilter):\n",
    "    \"\"\"\n",
    "    日本語の助詞/記号を除くフィルター\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def filter(cls,token):\n",
    "        \"\"\"\n",
    "        in: janome token\n",
    "        \"\"\"\n",
    "        stop_pos_list = (\"助詞\",\"副詞\",\"記号\")\n",
    "        if any([token.part_of_speech.startswith(pos) for pos in stop_pos_list]):\n",
    "            return None\n",
    "        return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01466a",
   "metadata": {},
   "source": [
    "# Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d26a35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    tokenizer = None\n",
    "    char_filters = []\n",
    "    token_filters = []\n",
    "    \n",
    "    @classmethod\n",
    "    def analyze(cls,text:str):\n",
    "        text = cls._char_filter(text)\n",
    "        tokens = cls.tokenizer.tokenize(text)\n",
    "        filtered_token = (cls._token_filter(token) for token in tokens)\n",
    "        return [parse_token(t) for t in filtered_token if t]\n",
    "    \n",
    "    @classmethod\n",
    "    def _char_filter(cls,text):\n",
    "        for char_filter in cls.char_filters:\n",
    "            text = char_filter.filter(text)\n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def _token_filter(cls,token):\n",
    "        for token_filter in cls.token_filters:\n",
    "            token = token_filter.filter(token)\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40bc66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapaneseAnalyzer(Analyzer):\n",
    "    tokenizer = JanomeTokenizer\n",
    "    char_filters = [HtmlStripFilter,LowercaseFilter]\n",
    "    token_filters = [StopWordFilter,POSFilter,Stemmer]\n",
    "    \n",
    "class EnglishAnalyzer(Analyzer):\n",
    "    tokenizer = WhitespaceTokenizer\n",
    "    char_filters = [HtmlStripFilter,LowercaseFilter]\n",
    "    token_filters = [StopWordFilter,POSFilter,Stemmer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5f28fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzed_query(parsed_query):\n",
    "    return_val = []\n",
    "    for q in parsed_query:\n",
    "        if q in OPRS:\n",
    "            return_val.append(q)\n",
    "        else:\n",
    "            analyzed_q = JapaneseAnalyzer.analyze(q)\n",
    "            if analyzed_q:\n",
    "                tmp = \" OR \".join(analyzed_q)\n",
    "                return_val += tmp.split(\" \")\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56797fd",
   "metadata": {},
   "source": [
    "# indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d88c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(\n",
    "        self,token_id: int, token:str,postings_list=[],docs_count=0)->None:\n",
    "        self.token_id = token_id\n",
    "        self.token = token\n",
    "        self.postings_list = []\n",
    "        self.__hash_handle = {}\n",
    "        self.docs_count = 0\n",
    "        \n",
    "def add_document(doc:str):\n",
    "    \"\"\"\n",
    "    ドキュメントをデータベースに追加し転置インデックスを構築する\n",
    "    \"\"\"\n",
    "    if not doc:\n",
    "        return \n",
    "    # # 文書IDと文書内容を基にミニ転置インデックス作成\n",
    "    text_to_postings_lists(doc)\n",
    "    \n",
    "    if len(TEMP_INVERT_INDEX) >=LIMIT:\n",
    "        for inverted_index in TEMP_INVERT_INDEX.values():\n",
    "            save_index(inverted_index)\n",
    "            \n",
    "def text_to_postings_lists(text)->list:\n",
    "    tokens = JapaneseAnalyzer.analyze(text)\n",
    "    token_count = len(tokens)\n",
    "    document_id = save_document(text,token_count)\n",
    "    \n",
    "    cnt = Counter(tokens)\n",
    "    for token, c in cnt.most_common():\n",
    "        token_to_posting_list(token,document_id,c)\n",
    "\n",
    "def text_to_posting_list(token:str,document:int,token_count:int):\n",
    "    token_id = get_token_id(token)\n",
    "    index = TEMP_INVERT_INDEX.get(token_id)\n",
    "    if not index:\n",
    "        index = InvertedIndex(token_id,token)\n",
    "    \n",
    "    posting = \"{}:{}\".format(str(document_id),str(token_count))\n",
    "    index.add_posting(posting)\n",
    "    \n",
    "    TEMP_INVERT_INDEX[token_id] = index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327b27d",
   "metadata": {},
   "source": [
    "# Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4077a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_query(query):\n",
    "    if not query:\n",
    "        return []\n",
    "    \n",
    "    #parse\n",
    "    parsed_query = tokenize(query)\n",
    "    parsed_query = analyzed_query(parsed_queqry)\n",
    "    rpn_tokens = parse_rpn(parsed_query)\n",
    "    \n",
    "    #merge\n",
    "    doc_ids,query_postings = merge(rpn_tokens)\n",
    "    print(doc_ids,query_postings)\n",
    "    \n",
    "    #fetch\n",
    "    docs = [fetch_doc(doc_id) for doc_id in doc_ids]\n",
    "    \n",
    "    #sort\n",
    "    sorted_docs = sort(docs,query_postings)\n",
    "    return [_parse_doc(doc) for doc, _ in sorted_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa90a7e",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf4d3cf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (153687985.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/4f/ggbfqk617fx1hgntsc54k7w00000gn/T/ipykernel_57151/153687985.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "REGEX_PATTERN = r\"\\s*(\\d+|\\w+|.)\"\n",
    "SPLITTER = re.compile(REGEX_PATTERN)\n",
    "\n",
    "LEFT = True\n",
    "RIGHT = False\n",
    "\n",
    "OPERATER = {\"AND\":(3,LEFT),\"OR\":(2,LEFT),\"NOT\":(1,RIGHT)}\n",
    "\n",
    "def tokennize(text):\n",
    "    return SPLITTER.findall(text)\n",
    "\n",
    "def parse_rpn(tokens:list):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b3166",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc2a84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(tokens:list):\n",
    "    target_posting = {}\n",
    "    \n",
    "    stack = []\n",
    "    for token in tokens:\n",
    "        if tokne not in OPRS:\n",
    "            token_id = get_token_id(token)\n",
    "            postings_list = fetch_postings_list(token_id)\n",
    "            \n",
    "            target_posting[token] = postings_list\n",
    "            \n",
    "            doc_ids = set([p[0] for p in postings_list])\n",
    "            stack.append(doc_ids)\n",
    "        \n",
    "        else:\n",
    "            if not stack:\n",
    "                raise\n",
    "            \n",
    "            if len(stack) == 1:\n",
    "                \n",
    "                if token ==  \"NOT\":\n",
    "                    \n",
    "                    return not_doc_ids,{}\n",
    "                else:\n",
    "                    raise\n",
    "            \n",
    "            doc_ids1 = stack.pop()\n",
    "            doc_ids2 = stack.pop()\n",
    "            stack.append(merge_posting(token,doc_ids1,doc_ids2))\n",
    "            \n",
    "def sort(doc_ids,query_postings):\n",
    "    docs = []\n",
    "    all_docs = count_all_docs()\n",
    "    for doc_id in doc_ids:\n",
    "        doc = fetch_doc(doc_id)\n",
    "        doc_tfidf = 0\n",
    "        for token,postings_list in query_postings.items():\n",
    "            idf = math.log10(all_docs/len(postings_list))+1\n",
    "            posting = [p for p in postings_list if p[0] ==doc.id]\n",
    "            if posting:\n",
    "                tf = round(posting[0][1]/doc.token_count,2)\n",
    "            else:\n",
    "                tf = 0\n",
    "                token_tfidf = tf*idf\n",
    "                doc_tfidf += token_tfidf\n",
    "            docs.append((doc,doc_tfidf))\n",
    "        return sorted(docs,key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600662f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
